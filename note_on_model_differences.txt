  1. Is this what Tartan_Federer does?                                                                    
                                                                                                          
  No, our approach is quite different from MIDST:                                                         
                                                                                                          
  ┌───────────────┬────────────────────────────────────────────────┬──────────────────────────────────┐   
  │               │             MIDST (Tartan_Federer)             │           Our approach           │   
  ├───────────────┼────────────────────────────────────────────────┼──────────────────────────────────┤   
  │ Noise vectors │ 1 random per timestep                          │ 300 fixed per sample             │
  ├───────────────┼────────────────────────────────────────────────┼──────────────────────────────────┤   
  │ Timesteps     │ All 0-999                                      │ Selected: [5,10,20,30,40,50,100] │   
  ├───────────────┼────────────────────────────────────────────────┼──────────────────────────────────┤
  │ Scoring       │ Loss ratio (synthetic model / auxiliary model) │ Raw losses → MLP classifier      │
  ├───────────────┼────────────────────────────────────────────────┼──────────────────────────────────┤
  │ Aggregation   │ Mean across feature dims per timestep          │ Flatten all 300×7 = 2100 losses  │
  └───────────────┴────────────────────────────────────────────────┴──────────────────────────────────┘

  MIDST computes loss_synthetic / loss_auxiliary per timestep — a ratio between two models. We instead
  train shadow models with known membership and let the MLP learn the signal from raw per-noise-vector
  losses. Fundamentally different philosophy.

  2. Do our config params match NoisyDiffusion?

  Yes, exact match on every parameter. Confirmed against
  /home/golobs/CAMDA25_NoisyDiffusion/TCGA-BRCA/config.yaml:

  - input_dim: 978, num_timesteps: 1000, hidden_dims: [2048, 2048], dropout: 0.2
  - time_embedding_dim: 128, label_embedding_dim: 64
  - attn_num_heads: 0, attn_num_tokens: 64, num_groups: 8
  - beta_schedule: "linear", linear_beta_start: 0.001, linear_beta_end: 0.02

  All match. The ND config also has alternative schedule params (cosine, power) but uses linear like us.
  COMBINED is identical except num_classes: 12.
